{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b877b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:00:10.414838: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 11:00:10.683811: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-18 11:00:11.779261: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2023-09-18 11:00:11.779538: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2023-09-18 11:00:11.779557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from utils.alloys import alloy_to_1d_tensor, get_elements_and_compositions, element_to_index\n",
    "from utils.constants import alloy_max_len, n_elements\n",
    "from utils.ml import *\n",
    "from utils.dataframes import dfs_tabs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa195676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (1269, 3)\n",
      "Testing set shape: (318, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train_final_df.csv\")\n",
    "df_test = pd.read_csv(\"test_final_df.csv\")\n",
    "\n",
    "# Print the shape of the training and testing sets to check the split sizes\n",
    "print(\"Training set shape:\", df_train.shape)\n",
    "print(\"Testing set shape:\", df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ac64b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.0\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ideally a max factor is from the training dataset\n",
    "max_factor = max(df_train[\"actual_d_max\"])\n",
    "print(max_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a10af4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1269, 20, 118]) torch.float32 torch.Size([1269]) 0.0 1.0\n",
      "torch.Size([318, 20, 118]) 0.0 0.8571428656578064\n"
     ]
    }
   ],
   "source": [
    "# creating the input and output data for the model\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "elements_vector_df = pd.read_csv(\"utils/elements_wvmodel_2016_12-15-20.csv\")\n",
    "\n",
    "def alloy_to_vectorized_tensor(alloy_str, alloy_max_len = alloy_max_len):\n",
    "    tensor = torch.zeros(20, 118)\n",
    "    elements, compositions = get_elements_and_compositions(alloy_str)\n",
    "    i = 0\n",
    "    for idx in range(0, len(elements) + len(compositions), 2):\n",
    "        tensor[idx, element_to_index(elements[i])] = 1\n",
    "        tensor[idx+1, int(compositions[i])] = 1\n",
    "        # print(element_to_index(elements[i]), int(compositions[i]))\n",
    "        i += 1\n",
    "    return tensor\n",
    "\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "print(alloy_to_vectorized_tensor(\"B6.0Cr16.0Fe78.0\"))\n",
    "torch.set_printoptions(profile=\"default\")\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    X_train.append(alloy_to_vectorized_tensor(df_train.loc[i, \"bmg_alloy\"]))\n",
    "    y_train.append(df_train.loc[i, \"actual_d_max\"] / max_factor)\n",
    "    \n",
    "X_train = torch.stack(X_train)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape, X_train.dtype, y_train.shape, min(y_train).item(), max(y_train).item())\n",
    "\n",
    "for i in range(df_test.shape[0]):\n",
    "    X_test.append(alloy_to_vectorized_tensor(df_test.loc[i, \"bmg_alloy\"]))\n",
    "    y_test.append(df_test.loc[i, \"actual_d_max\"] / max_factor)\n",
    "    \n",
    "X_test = torch.stack(X_test)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(X_test.shape, min(y_test).item(), max(y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7032c90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [2.3641e-03, 1.1820e-02, 4.0189e-02,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        ...,\n",
      "        [1.0000e-08, 1.0000e-08, 3.9401e-03,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08]]) tensor([[1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [4.8583e-02, 1.0812e-01, 1.9648e-01,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        ...,\n",
      "        [1.0000e-08, 1.0000e-08, 6.2671e-02,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08],\n",
      "        [1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming X_train and X_test are Python lists of data\n",
    "# Convert them to PyTorch tensors\n",
    "X_train = torch.tensor(X_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "\n",
    "# Calculate mean and standard deviation from the training data along axis 0\n",
    "mean = torch.mean(X_train, dim=0)\n",
    "std = torch.std(X_train, dim=0)\n",
    "\n",
    "epsilon = 1e-8  # You can adjust this value if needed\n",
    "std = std + epsilon\n",
    "mean = mean + epsilon\n",
    "print(mean, std)\n",
    "\n",
    "# Normalize the data\n",
    "normalized_X_train = (X_train - mean) / std\n",
    "normalized_X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9f9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]], shape=(1269, 20, 118), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]\n",
      "\n",
      " [[False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  ...\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]\n",
      "  [False False False ... False False False]]], shape=(318, 20, 118), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:00:17.955639: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-09-18 11:00:17.955742: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (laptop): /proc/driver/nvidia/version does not exist\n",
      "2023-09-18 11:00:17.957567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "is_nan_train = tf.math.is_nan(normalized_X_train)\n",
    "print(is_nan_train)\n",
    "\n",
    "is_nan_test = tf.math.is_nan(normalized_X_test)\n",
    "print(is_nan_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ab78f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5000, Loss: 0.6993\n",
      "Epoch 100/5000, Loss: 0.3200\n",
      "Epoch 200/5000, Loss: 0.3200\n",
      "Epoch 300/5000, Loss: 0.3200\n",
      "Epoch 400/5000, Loss: 0.3200\n",
      "Epoch 500/5000, Loss: 0.3200\n",
      "Epoch 600/5000, Loss: 0.3200\n",
      "Epoch 700/5000, Loss: 0.3200\n",
      "Epoch 800/5000, Loss: 0.3200\n",
      "Epoch 900/5000, Loss: 0.3200\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the output of the last time step\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Define the model parameters\n",
    "input_size = 118\n",
    "hidden_size = 128\n",
    "num_layers = 10\n",
    "output_size = 1\n",
    "\n",
    "# Create the model\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "X_train = torch.tensor(normalized_X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(normalized_X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_train = y_train.view(-1, 1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "y_test = y_test.view(-1, 1)\n",
    "# Training loop\n",
    "num_epochs = 5000\n",
    "print_every = 100\n",
    "plot_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    plot_loss.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(plot_loss)), plot_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"rnn_trainset_trained_method_4.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once training is done, you can use the trained model to make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_labels = model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b54301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(np.array(y_test)*max_factor, np.array(predicted_labels).reshape(-1)*max_factor)\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b007464",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=np.array(y_test)*max_factor, y=np.array(predicted_labels).reshape(-1)*max_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rnn_train_output = []\n",
    "for i in range(df_train.shape[0]):\n",
    "    alloy_tensor = alloy_to_vectorized_tensor(df_train.loc[i, \"bmg_alloy\"])\n",
    "    output = model(alloy_tensor.unsqueeze(0))\n",
    "    all_rnn_train_output.append(output.squeeze().item() * max_factor)\n",
    "\n",
    "all_rnn_test_output = []\n",
    "for i in range(df_test.shape[0]):\n",
    "    alloy_tensor =  alloy_to_vectorized_tensor(df_test.loc[i, \"bmg_alloy\"])\n",
    "    output = model(alloy_tensor.unsqueeze(0))\n",
    "    all_rnn_test_output.append(output.squeeze().item() * max_factor)\n",
    "    \n",
    "new_train = df_train\n",
    "new_test = df_test\n",
    "\n",
    "\n",
    "new_train[\"rnn_encoding\"] = all_rnn_train_output\n",
    "new_test[\"rnn_encoding\"] = all_rnn_test_output\n",
    "    \n",
    "dfs = [new_train, new_test]\n",
    "sheets = [\"train\", \"test\"]\n",
    "dfs_tabs(dfs, sheets, 'dataset_rnn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"dataset_rnn.xlsx\", sheet_name=\"train\")\n",
    "df_test = pd.read_excel(\"dataset_rnn.xlsx\", sheet_name=\"test\")\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"dataset_rnn.xlsx\", index_col=0, sheet_name=\"train\")\n",
    "df_test = pd.read_excel(\"dataset_rnn.xlsx\", index_col=0, sheet_name=\"test\")\n",
    "# df_test = df_test.drop([\"predicted_d_max\"], axis=1)\n",
    "df_train = df_train.drop([\"paper_sno\"], axis=1)\n",
    "df_test = df_test.drop([\"paper_sno\"], axis=1)\n",
    "X_train, y_train = df_train.loc[:, df_train.columns != \"actual_d_max\"], pd.DataFrame(df_train[\"actual_d_max\"])\n",
    "X_test, y_test = df_test.loc[:, df_test.columns != \"actual_d_max\"], pd.DataFrame(df_test[\"actual_d_max\"])\n",
    "\n",
    "R2 = []\n",
    "ADJR2 = []\n",
    "RMSE = []\n",
    "names = []\n",
    "TIME = []\n",
    "\n",
    "for name, model in tqdm(REGRESSORS):\n",
    "    start = time.time()\n",
    "    pipe = Pipeline(steps=[\n",
    "                        (\"classifier\", model()),\n",
    "                    ]\n",
    "                )\n",
    "    try:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        r_squared = r2_score(y_test, y_pred)\n",
    "        print(name, r_squared)\n",
    "        adj_rsquared = adjusted_rsquared(\n",
    "            r_squared, X_test.shape[0], X_test.shape[1]\n",
    "        )\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        names.append(name)\n",
    "        R2.append(r_squared)\n",
    "        ADJR2.append(adj_rsquared)\n",
    "        RMSE.append(rmse)\n",
    "        TIME.append(time.time() - start)\n",
    "    except Exception as exception:\n",
    "        print(name + \" model failed to execute\")\n",
    "        print(exception)\n",
    "    scores = {\n",
    "                \"Model\": names,\n",
    "                \"Adjusted R-Squared\": ADJR2,\n",
    "                \"R-Squared\": R2,\n",
    "                \"RMSE\": RMSE,\n",
    "                \"Time Taken\": TIME,\n",
    "            }\n",
    "    scores = pd.DataFrame(scores)\n",
    "    scores = scores.sort_values(by = \"Adjusted R-Squared\", ascending = False).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4edbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"dataset_rnn.xlsx\", index_col=0, sheet_name=\"train\")\n",
    "df_test = pd.read_excel(\"dataset_rnn.xlsx\", index_col=0, sheet_name=\"test\")\n",
    "# df_test = df_test.drop([\"predicted_d_max\"], axis=1)\n",
    "df_train = df_train.drop([\"paper_sno\"], axis=1)\n",
    "df_test = df_test.drop([\"paper_sno\"], axis=1)\n",
    "X_train, y_train = df_train.loc[:, df_train.columns != \"actual_d_max\"], pd.DataFrame(df_train[\"actual_d_max\"])\n",
    "X_test, y_test = df_test.loc[:, df_test.columns != \"actual_d_max\"], pd.DataFrame(df_test[\"actual_d_max\"])\n",
    "\n",
    "R2 = []\n",
    "ADJR2 = []\n",
    "RMSE = []\n",
    "names = []\n",
    "TIME = []\n",
    "\n",
    "for transformer_method_name, transformer_method in tqdm(TRANSFOMER_METHODS):\n",
    "    for name, model in tqdm(REGRESSORS):\n",
    "        start = time.time()\n",
    "        X_transformer = transformer_method()\n",
    "        y_transformer = transformer_method()\n",
    "        transformed_X_train = pd.DataFrame(X_transformer.fit_transform(X_train), columns = X_train.columns)\n",
    "        transformed_X_test = pd.DataFrame(X_transformer.transform(X_test), columns = X_test.columns)\n",
    "\n",
    "        transformed_y_train = pd.DataFrame(y_transformer.fit_transform(y_train), columns = y_train.columns)\n",
    "        transformed_y_test = pd.DataFrame(y_transformer.transform(y_test), columns = y_test.columns)\n",
    "        pipe = Pipeline(steps=[\n",
    "                            (\"classifier\", model()),\n",
    "                        ]\n",
    "                    )\n",
    "        try:\n",
    "            pipe.fit(transformed_X_train, transformed_y_train)\n",
    "            transformed_y_pred = pipe.predict(transformed_X_test)\n",
    "            r_squared = r2_score(transformed_y_test, transformed_y_pred)\n",
    "            adj_rsquared = adjusted_rsquared(\n",
    "                r_squared, transformed_X_test.shape[0], transformed_X_test.shape[1]\n",
    "            )\n",
    "            rmse = np.sqrt(mean_squared_error(transformed_y_test, transformed_y_pred))\n",
    "            names.append(name + \" (\" + transformer_method_name + \")\")\n",
    "            print(name + \" (\" + transformer_method_name + \")\", r_squared)\n",
    "            R2.append(r_squared)\n",
    "            ADJR2.append(adj_rsquared)\n",
    "            RMSE.append(rmse)\n",
    "            TIME.append(time.time() - start)\n",
    "        except Exception as exception:\n",
    "            print(name + \" (\" + transformer_method_name + \")\" + \" model failed to execute\")\n",
    "            print(exception)\n",
    "        scores = {\n",
    "                    \"Model\": names,\n",
    "                    \"Adjusted R-Squared\": ADJR2,\n",
    "                    \"R-Squared\": R2,\n",
    "                    \"RMSE\": RMSE,\n",
    "                    \"Time Taken\": TIME,\n",
    "                }\n",
    "        scores = pd.DataFrame(scores)\n",
    "        scores = scores.sort_values(by = \"Adjusted R-Squared\", ascending = False).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scores[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4601285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
